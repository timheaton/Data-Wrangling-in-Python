#Part 1, create files for each database table

database = 'CCIS_SLDS_Staging' #the database to be consumed NOTE: This has to be changed for each Database
output_directory = ("M:/Tim Heaton/SLDS/Data Dictionary/All_Tables/") #where you want the files placed

# Do not change anything below this line

import pymssql
import pandas as pd
import numpy as np
#from pydqc import infer_schema, data_summary, data_compare, data_consist
import os
file_type = (".csv")

## instance a python db connection object- same form as psycopg2/python-mysql drivers also
conn = pymssql.connect(server="warehouse", user="theaton",password="G0Dawg52019!!", port="1433")  # You can lookup the port number inside SQL server. 

stmt = "SELECT * FROM " + database + ".INFORMATION_SCHEMA.TABLES WHERE TABLE_TYPE = 'BASE TABLE'"

# Excute Query to find all the columns
tables = pd.read_sql(stmt,conn)

for i in tables["TABLE_NAME"]:
    stmt = "SELECT * FROM " + database + ".[dbo].["+i+"]"

# Query that samples the rows 
    table_data = pd.read_sql(stmt,conn)
    sample_data = table_data.head(100) #take the first 100 rows
    df = sample_data

#building the individual DQR reports 
    columns = pd.DataFrame(list(df.columns.values))
    data_types = pd.DataFrame(df.dtypes, columns=['Data Type'])
    missing_data_counts = pd.DataFrame(df.isnull().sum(), columns=['Missing Values'])
    present_data_counts = pd.DataFrame(df.count(), columns=['Present Values'])

    sample_value = pd.DataFrame(columns=['Sample Value'])
    for v in list(df.columns.values):
        try:
            sample_value.loc[v] = [df[v].sample(n=3, random_state=1)]
        except ValueError:
            print (v + "_ValueError")

#export the individual DQRs

    data_quality_report = data_types.join(present_data_counts).join(missing_data_counts).join(sample_value)

    data_quality_report.to_csv(output_directory+'/'+database+".dbo."+i+'.csv')



# Part 2, After all the DQRs for all databases have been run, this script stacks them into one file
# Make sure to remove the "x" in the "not used" column

folder = 'M:/Tim Heaton/SLDS/Data Dictionary/All_Tables'
#"column names" should not be hard-coded!
column_names = ('Field_Name','Data Type', 'Present Values', 'Missing Values', 'Sample Value') 
outputfile = 'M:/Tim Heaton/SLDS/Data Dictionary/MergedDict/'
outputfile_name = 'SLDS_Codebooks_ForNormalizer'

import pandas as pd
import os
for subdir, dirs, files in os.walk(folder):
    for file in files:
        appended_data = []
        for file in files:
            data = pd.read_csv(folder+"/"+file, header = None, skiprows=[0])
            data.columns = column_names
            data["Table_Name"] =  file[:-4]
            # store DataFrame in list
            appended_data.append(data)
        # see pd.concat documentation for more info
        appended_data = pd.concat(appended_data, axis=0)
        appended_data['not used'] = 'x'
        appended_data.to_csv(outputfile + outputfile_name +".csv",index=False)



# Part 3 choose columns based on spreadsheet, convert times to date only, fill in blanks
import pymssql
import numpy as np
import pandas as pd

#INPUTS ONLY ON THESE TWO LINES
DataMart_Tables = pd.read_csv('M:/Tim Heaton/SLDS/Data Dictionary/MergedDict/SLDS_Codebooks_ForNormalizer.csv')
New_Table_Name_Suffix = 'NormalizerTest_'


#DON'T CHANGE BELOW
DataMart_Tables = DataMart_Tables[(DataMart_Tables['not used']!='x')] #all the needed fields are marked by "x" in the file
DataMart_Tables_Subset = DataMart_Tables[["Field_Name","Table_Name"]] #just the columns we need
table_names = DataMart_Tables_Subset['Table_Name'].unique().tolist() #unique table names
table_names_pd = pd.DataFrame(table_names)#forgot to delete this or I'm afraid to



for i in table_names:
    tables = DataMart_Tables_Subset.loc[(DataMart_Tables_Subset['Table_Name'] == i)]#find the columns for each table
    column_names = tables['Field_Name'] #1-modify tables['Field_Name'] to use in the query, must be a string
    column_names = column_names.astype(str).values.tolist()#2-modify tables['Field_Name'] for query, must be a string
    column_names = ', '.join(column_names)#3-modify tables['Field_Name'] to use in the query, must be a string
    
    database_table = i # interation will be the name of the ".dbo."
    #New_table_name = New_Table_Name_Suffix + i #the table to be updated with the data

    
    conn = pymssql.connect(server="warehouse", user="theaton",password="G0Dawg52019!!", port="1433")  
    try:
        stmt = "SELECT " + column_names + " FROM " + database_table
        # Excute Query here
        db_table = pd.read_sql(stmt,conn)
        db_table = db_table.replace(r'^\s*$', np.nan, regex=True)#replace all empty fields with NaN (will be "NULL" on SQL import)
        #date consistant

        date_columns = db_table.select_dtypes(include=['datetime64']).head()
        for each_column in  date_columns.columns:
            #print(df[i].dt.date)
            db_table[each_column] = db_table[each_column].dt.date
        
    
    
    
        



        #Code below is to export to csv file    
        #db_table.to_csv("M:/Tim Heaton/SLDS/Data Dictionary/MergedDict/"+ i + ".csv")
        
        #SQL table update code block, 
        from sqlalchemy import *
        import pyodbc
        New_table_name = New_Table_Name_Suffix + i #the table to be updated with the data
        #engine = create_engine('mssql+pymssql://theaton:G0Dawg52019!!@warehouse:1433/EC_SLDS')
        engine = create_engine('mssql+pymssql://theaton:G0Dawg52019!!@warehouse:1433/Normalizer_Testing')
        # write the DataFrame to a table in the sql database
        db_table.to_sql(New_table_name, engine, if_exists = 'replace',index=False)
        
        
        print("Completed " + i)
    
    except:
        print("Error in " + database_table)
        #print(db_table[each_column])
        pass

    
